<!DOCTYPE html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  class="html"
><head>
  <title>
    
      
        Gradient Descent: Newton and Quasi-Newton Method |

      
      James Shihua


    
  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.95.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta
    name="description"
    content="
      Gradient descent methods and their improvements.


    "
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.422eb4f2d656dfb45634538c41d755f199bfe27df14464c5fbc2c99ea6e889f7.css"
      integrity="sha256-Qi608tZW37RWNFOMQddV8Zm/4n3xRGTF&#43;8LJnqboifc="
      crossorigin="anonymous"
      type="text/css"
    />

  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css"
    integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.69685904d5c2a0c6258d03c207b778c10466edf6cea928cc0164c376b0ad0930.css"
    integrity="sha256-aWhZBNXCoMYljQPCB7d4wQRm7fbOqSjMAWTDdrCtCTA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.6d585e78d28cce1200d39fb133c92ed83df01738da721d0f48fb6eac62d24e04.css"
    integrity="sha256-bVheeNKMzhIA05&#43;xM8ku2D3wFzjach0PSPturGLSTgQ="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.6b00d96498d59caa0dbcf9b49d30d821915291f2ceb0e19248523c8607ff43fa.css"
    integrity="sha256-awDZZJjVnKoNvPm0nTDYIZFSkfLOsOGSSFI8hgf/Q/o="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.icofavicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/favicon.icoapple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon.icofavicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon.icofavicon-16x16.png" />

  <link rel="canonical" href="https://jamesmshihua.com/post/gd-quasi-newton/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js"
      integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8="
      crossorigin="anonymous"
    ></script>

  

  


  
  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jamesmshihua.com/images/site-feature-image.png"/>

<meta name="twitter:title" content="Gradient Descent: Newton and Quasi-Newton Method"/>
<meta name="twitter:description" content="Gradient descent methods and their improvements."/>



  
  <meta property="og:title" content="Gradient Descent: Newton and Quasi-Newton Method" />
<meta property="og:description" content="Gradient descent methods and their improvements." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jamesmshihua.com/post/gd-quasi-newton/" /><meta property="og:image" content="https://jamesmshihua.com/images/site-feature-image.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-04-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-04-08T00:00:00+00:00" /><meta property="og:site_name" content="My blog" />
<meta property="og:see_also" content="https://jamesmshihua.com/post/sailboat-path-opt/" /><meta property="og:see_also" content="https://jamesmshihua.com/post/gd-linesearch/" />




  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Gradient Descent: Newton and Quasi-Newton Method",
        "headline": "Gradient Descent: Newton and Quasi-Newton Method",
        "alternativeHeadline": "",
        "description": "
      Gradient descent methods and their improvements.


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/jamesmshihua.com\/post\/gd-quasi-newton\/"
        },
        "author" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "creator" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "copyrightYear" : "2022",
        "dateCreated": "2022-04-08T00:00:00.00Z",
        "datePublished": "2022-04-08T00:00:00.00Z",
        "dateModified": "2022-04-08T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "James Shihua",
            "url": "https://jamesmshihua.com",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/jamesmshihua.com\/favicons\/favicon.icofavicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "https://jamesmshihua.com/images/site-feature-image.png"


      
      ]

    ,
        "url" : "https:\/\/jamesmshihua.com\/post\/gd-quasi-newton\/",
        "wordCount" : "1034",
        "genre" : [ ],
        "keywords" : [ 
      
      "optimisation"

    
      
        ,

      
      "mathematics"

    ]
    }
  </script>



</head>
<body
    
      class="body theme--light"

    
  >
    <div class="wrapper">
      <aside class="wrapper__sidebar"><div
  class="sidebar
    animated fadeInDown

  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      <h1 class="sidebar__introduction-title">
        <a href="/">My blog</a>
      </h1>
      <div class="sidebar__introduction-description">
        <p>James's imagination runs wild</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a href="https://www.linkedin.com/in/james-mingzhi-shihua-b4897a173/" rel="me" aria-label="Linkedin" title="Linkedin">
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="https://github.com/jamesmshihua/" rel="me" aria-label="GitHub" title="GitHub">
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="https://www.instagram.com/jameshmz/" rel="me" aria-label="instagram" title="instagram">
            <i class="fab fa-instagram fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="mailto:ampule_dignity.05@icloud.com" rel="me" aria-label="e-mail" title="e-mail">
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        James Shihua
        2024


      
    </li>
    
      <li class="footer__item">
        <a
          class="link"
          href="/imprint/"
          
          title=""
        >
          imprint
        </a>
      </li>

    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.a3fd0638ada4e3cec287e49a604b4a828cfef33a669afe60b96575761fff5cf3.js"
    integrity="sha256-o/0GOK2k487Ch&#43;SaYEtKgoz&#43;8zpmmv5guWV1dh//XPM="
    crossorigin="anonymous"
  ></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] },
        TeX: { equationNumbers: { autoNumber: "all" } }
    });
    </script></div>
</aside>
      <main class="wrapper__main">
        <header class="header"><div
  class="
    animated fadeInDown

  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        <li class="nav__list-item">
          <a
            
            href="/"
            
            title=""
            >Home</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/post/"
            
            title=""
            >Posts</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/portfolio/"
            
            title=""
            >Portfolio</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/about/"
            
            title=""
            >About</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/contact/"
            
            title=""
            >Contact</a
          >
        </li>

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>

      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown

    "
  >
    
    <div class="post__content">
      <h1>Gradient Descent: Newton and Quasi-Newton Method</h1>
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                8/4/2022


              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">5-minute read</span>
          </li>
        </ul>

      <h2 id="newtons-method">Newton&rsquo;s Method</h2>
<p>Different from gradient-descent (GD) method which tries to directly converge the design variable to a minimiser, <strong>Newton&rsquo;s method</strong> uses an important characteristic of an extreme value &ndash; gradient of the function ($\nabla \mathbf{f}$) equals zero. While GD has a first-order convergence, Newton&rsquo;s method has second-order convergence, which means it suffers less severe zig-zag problems.</p>
<p>Recall our primary school math, that we can find the root of an equation $f=0$ by
$$
x_{k+1} = x_k - \frac{f}{f^\prime}.
$$</p>
<p>Similarly, we can find the root of $f^\prime=0$ by
$$
x_{k+1} = x_k - \frac{f^\prime}{f^{\prime\prime}}.
\label{eqn:-1}
$$</p>
<p>Rearrange \eqref{eqn:-1} and extend to higher dimension, and we can obtain
$$
\mathbf{H}\Delta\mathbf{x_k}=-\nabla\mathbf{f}\left(\mathbf{x} _{k}\right).
\label{eqn:-2}
$$
\eqref{eqn:-2} is the update formula for Newton&rsquo;s method.</p>
<h2 id="quasi-newton-method">Quasi-Newton Method</h2>
<p>In Newton&rsquo;s method, the <em>inverse of Hessian</em> ($\mathbf{H}^{-1}$) is required. For a large-scale optimisation problem, the number of variables ($n$) can be very large, and the size of the Hessian ($n\times n$) is even larger. Inverting the Hessian matrix can be thus very expensive. In <strong>Quasi-Newton method</strong>, the <em>Hessian is approximated</em> instead of directly computed, by an iterative process, which is much computationally cheaper.</p>
<h3 id="lemma-broyden-root-finding-method">Lemma: Broyden Root-Finding Method</h3>
<p>For scalar functions ($\mathbb{R}\to\mathbb{R}$), it is possible to approximate the gradient of a function by solving
$$
\mathbf{J} _{k}\left(\mathbf{x} _{k}-\mathbf{x} _{k-1}\right)=\mathbf{F}\left(\mathbf{x} _{k}\right)-\mathbf{F}\left(\mathbf{x} _{k-1}\right).
\label{eqn:1}
$$
where $J _{k}$ is the approximated gradient.</p>
<p>However, this does not work in vector-valued functions ($\mathbb{R}^n\to\mathbb{R}^n$), because we are solving $\mathbf{J}_{k}$, an $n\times n$ matrix, by only $n$ equations. Graphically, we cannot find the gradient in all directions by only draw one secant line of the function.</p>
<p>However, we still want to find a way to approximate $\mathbf{J}_{k}$ iteratively, if the straightforward approach is impossible. The goal is to solve
$$
\mathbf{J} _{k} \mathbf{s} _{k}=\Delta \mathbf{F} _{k},
\label{eqn:2}
$$
where $\mathbf{s} _{k}=\mathbf{x} _{k}-\mathbf{x} _{k-1}$ and $\Delta \mathbf{F} _{k}=\mathbf{F}\left(x _{k}\right)-\mathbf{F}\left(x _{k-1}\right)$.</p>
<p>One way to update $\mathbf{J} _{k}$ is
$$
\mathbf{J} _{k}=\mathbf{J} _{k-1}+\frac{\Delta \mathbf{F} _{k}-\mathbf{J} _{k-1} \mathbf{s} _{k}}{\left|\mathbf{s} _{k}\right|^{2}} \mathbf{s} _{k}^{\top}.
\label{eqn:3}
$$</p>
<p>It can be seen that \eqref{eqn:2} and \eqref{eqn:3} are consistent. In addition, \eqref{eqn:3} tries to minimise the Frobenius norm (elementwise L2 norm) of $\left|\mathbf{J} _{k}-\mathbf{J} _{k-1}\right| _F$. Once we have $\mathbf{J} _{k}$, we can update $\mathbf{x} _{k}$ by \eqref{eqn:1}.</p>
<p>However, note that only $\mathbf{J}^{-1} _{k}$ is required in \eqref{eqn:1} and inverting the Jacobian at each iteration is expensive for a large $n$. It is desired to update $\mathbf{J}^{-1} _{k}$ directly.</p>
<p>Here, we apply the Sherman-Morrison formula, which reads
$$
\left(A+u v^{\top}\right)^{-1}=A^{-1}-\frac{A^{-1} u v^{\top} A^{-1}}{1+v^{\top} A^{-1} u}.
$$</p>
<p>We can now express \eqref{eqn:1} by
$$
\mathbf{J} _{k}^{-1}=\mathbf{J} _{k-1}^{-1}+\frac{\Delta \mathbf{x} _{k}-\mathbf{J} _{k-1}^{-1} \Delta F _{k}}{\Delta \mathbf{x} _{k}^{\top} \mathbf{J} _{k-1}^{-1} \Delta F _{k}} \Delta \mathbf{x} _{k}^{\top} \mathbf{J} _{k-1}^{-1}.
\label{eqn:4}
$$</p>
<p>We will store $\mathbf{J} _{k-1}^{-1}$ in the memory and update $\mathbf{J} _{k}^{-1}$ by evaluating \eqref{eqn:4} repeatedly.</p>
<h3 id="broydenfletchergoldfarbshanno-bfgs-algorithm">Broyden–Fletcher–Goldfarb–Shanno (BFGS) Algorithm</h3>
<p>Similar to the Broyden root-finding method, BFGS method approximates Hessian in \eqref{eqn:-2} by
$$
\mathbf{B} _{k} \mathbf{s} _{k}=-\nabla \mathbf{f}\left(\mathbf{x} _{k}\right)
$$
where $\mathbf{B} _{k}$ is the approximated Hessian, $\mathbf{s} _{k}$ is the step, and RHS is the gradient of the objective function.</p>
<p>First, we need an initial guess for $\mathbf{B} _{k}$ so that we can calculate $\mathbf{s} _{k}$ &ndash; this linear system is deterministic. Then, we can update $\mathbf{x}$ by
$$
\mathbf{x} _{k+1}=\mathbf{x} _{k}+\mathbf{S} _{k}.
$$</p>
<p>We also denote the change of gradient
$$
\mathbf{y} _{k}=\nabla \mathbf{f}\left(\mathbf{x} _{k+1}\right)-\nabla \mathbf{f}\left(\mathbf{x} _{k}\right).
$$</p>
<p>Similar to \eqref{eqn:2}, we need to choose an update formula for $\mathbf{B} _{k}$ to be
$$
\mathbf{B} _{k+1} \mathbf{s} _{k}=\mathbf{y} _{k}.
\label{eqn:5}
$$</p>
<p>To ensure $\mathbf{B}_k$ is symmetric positive definite (SPD), we can enforce the following update rule
$$
\mathbf{B} _{k+1}=\mathbf{B} _{k}+\alpha \mathbf{u}^{} \mathbf{u}^{\top}+\beta \mathbf{v}^{} \mathbf{v}^{\top}.
\label{eqn:6}
$$</p>
<p>For \eqref{eqn:6} to satisfy \eqref{eqn:5}, we try $\mathbf{u}=\mathbf{y} _{k}$ and $\mathbf{v}=\mathbf{B} _{k} \mathbf{s} _{k}$ and see whether we can find a possible $\alpha$ and $\beta$ value. Now, \eqref{eqn:6} becomes
$$
\mathbf{B} _{k+1}=\mathbf{B} _{k}+\alpha \mathbf{y} _{k} \mathbf{y} _{k}^{\top}+\beta \mathbf{B} _{k} \mathbf{s} _{k} \mathbf{s} _{k}^{\top} \mathbf{B} _{k}.
$$</p>
<p>To be consistent with \eqref{eqn:5}, we need
$$
\color{CadetBlue}{\mathbf{y} _{k}}=\mathbf{B} _{k+1} \mathbf{s} _{k}=\color{DarkSalmon}{\mathbf{B} _{k} \mathbf{s} _{k}}+\color{CadetBlue}{\alpha \mathbf{y} _{k} \mathbf{y} _{k}^{\top} \mathbf{s} _{k}}+\color{DarkSalmon}{\beta \mathbf{B} _{k} \mathbf{s} _{k} \mathbf{s} _{k}^{\top} \mathbf{B} _{k} \mathbf{s} _{k}}.
$$</p>
<p>Matching terms in the same colour and we obtain
$$
\alpha=\frac{1}{\mathbf{y} _{k}^{\top} \mathbf{s} _{k}}, \quad \beta=-\frac{1}{\mathbf{s} _{k}^{\top} \mathbf{B} _{k} \mathbf{s} _{k}}.
$$</p>
<p>Hence, the update formula for approximated Hessian is
$$
\mathbf{B} _{k+1}=\mathbf{B} _{k}+\frac{\mathbf{y} _{k} \mathbf{y} _{k}^{\top}}{\mathbf{y} _{k}^{\top} \mathbf{s} _{k}}-\frac{\mathbf{B} _{k} .\mathbf{s} _{k} \mathbf{s} _{k}^{\top} \mathbf{B} _{k}}{\mathbf{s} _{k}^{\top} \mathbf{B} _{k} \mathbf{s} _{k}}.
$$</p>
<p>Similarly, we also want to update the $\mathbf{B}^{-1} _{k}$ if possible. Using Sherman-Morrison formula again, without a step-by-step derivation, we reach
$$
\Delta \mathbf{H} _{k}=\left(\mathbf{I}-\mathbf{s} _{k} \rho _{k} \mathbf{y} _{k}^{\top}\right) \mathbf{H} _{k}\left(\mathbf{I}-\rho _{k} \mathbf{y} _{k} \mathbf{s} _{k}^{\top}\right)+\rho _{k} \mathbf{s} _{k} \mathbf{s} _{k}^{\top}
$$
where $\rho _{k}=\dfrac{1}{\mathbf{y} _{k}^{\top} \mathbf{s} _{k}}$ is a scalar.</p>
<h2 id="a-sample-study">A Sample Study</h2>
<p>The above mentioned methods are implemented in a Julia package <code>Optim.jl</code>. Here we use this package to optimise the Rosenbrock function
$$
f(x,y) = (1 - x)^2 + 100.0 (y - x^2)^2.
$$</p>
<p>We use Gradient Descent method (introduced in the previous blog), both with and without line search, and BFGS optimiser. The results is shown in the following figure:
<figure class="middle"><img src="/images/gdm2/BFGS.png"
         alt="image"/>
</figure>
</p>
<p>It can be clearly seen that BFGS performs the best, converging to the global minimum $(1,1)$ with 26 iterations. GD with line search requires 615 iterations, a significant larger number, also converging to the correct value. However, the vanilla GD method uses 1000 iterations and still cannot converge to the minimum.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Optim</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Plots</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">LineSearches</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="kt">LinRange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="kt">LinRange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">&#34;Rosenbrock&#34;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">&#34;x&#34;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">&#34;y&#34;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="ss">:topleft</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">contour!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">f</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]),</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">start</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">res1</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">BFGS</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">	<span class="n">Optim</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">		<span class="n">store_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">extended_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">trace1</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="p">(</span><span class="n">permutedims</span><span class="p">,</span> <span class="n">vcat</span><span class="p">,</span> <span class="n">Optim</span><span class="o">.</span><span class="n">x_trace</span><span class="p">(</span><span class="n">res1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plot!</span><span class="p">(</span><span class="n">trace1</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">trace1</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="ss">:circle</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#34;BFGS (</span><span class="si">$</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">trace1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="s"> iterations)&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markeralpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markerstrokewidth</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">algo_gd_hz</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(;</span><span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialStatic</span><span class="p">(),</span> <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">HagerZhang</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">res2</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">algo_gd_hz</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">Optim</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">		<span class="n">store_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">extended_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">trace2</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="p">(</span><span class="n">permutedims</span><span class="p">,</span> <span class="n">vcat</span><span class="p">,</span> <span class="n">Optim</span><span class="o">.</span><span class="n">x_trace</span><span class="p">(</span><span class="n">res2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plot!</span><span class="p">(</span><span class="n">trace2</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">trace2</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="ss">:circle</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#34;GD w/ Line Search (</span><span class="si">$</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">trace2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="s"> iterations)&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markeralpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markerstrokewidth</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">res3</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">GradientDescent</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">	<span class="n">Optim</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">		<span class="n">store_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">extended_trace</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">trace3</span> <span class="o">=</span> <span class="n">mapreduce</span><span class="p">(</span><span class="n">permutedims</span><span class="p">,</span> <span class="n">vcat</span><span class="p">,</span> <span class="n">Optim</span><span class="o">.</span><span class="n">x_trace</span><span class="p">(</span><span class="n">res3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plot!</span><span class="p">(</span><span class="n">trace3</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">trace3</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="ss">:circle</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#34;GD (</span><span class="si">$</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">trace3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="s"> iterations)&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markeralpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">markerstrokewidth</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span></span></code></pre></div>


<h3>Posts in this Series</h3>
<ul>
  
    <li><a href="/post/sailboat-path-opt/">How Fast Can a Sailboat Go: A Constrained Optimization Approach</a></li>

  
    <li><a href="/post/gd-quasi-newton/">Gradient Descent: Newton and Quasi-Newton Method</a></li>

  
    <li><a href="/post/gd-linesearch/">Gradient Descent: Line Search</a></li>

  
</ul>
</div>
    <div class="post__footer">
      

      
        <span><a class="tag" href="/tags/optimisation/">optimisation</a><a class="tag" href="/tags/mathematics/">mathematics</a></span>




      
    </div>

    
  </div>


      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        James Shihua
        2024


      
    </li>
    
      <li class="footer__item">
        <a
          class="link"
          href="/imprint/"
          
          title=""
        >
          imprint
        </a>
      </li>

    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.a3fd0638ada4e3cec287e49a604b4a828cfef33a669afe60b96575761fff5cf3.js"
    integrity="sha256-o/0GOK2k487Ch&#43;SaYEtKgoz&#43;8zpmmv5guWV1dh//XPM="
    crossorigin="anonymous"
  ></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] },
        TeX: { equationNumbers: { autoNumber: "all" } }
    });
    </script></body>
</html>
