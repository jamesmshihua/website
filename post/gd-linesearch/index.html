<!DOCTYPE html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  class="html"
><head>
  <title>
    
      
        Gradient Descent: Line Search |

      
      James Shihua


    
  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.95.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta
    name="description"
    content="
      Gradient descent methods and their improvements.


    "
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.422eb4f2d656dfb45634538c41d755f199bfe27df14464c5fbc2c99ea6e889f7.css"
      integrity="sha256-Qi608tZW37RWNFOMQddV8Zm/4n3xRGTF&#43;8LJnqboifc="
      crossorigin="anonymous"
      type="text/css"
    />

  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css"
    integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.69685904d5c2a0c6258d03c207b778c10466edf6cea928cc0164c376b0ad0930.css"
    integrity="sha256-aWhZBNXCoMYljQPCB7d4wQRm7fbOqSjMAWTDdrCtCTA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.6d585e78d28cce1200d39fb133c92ed83df01738da721d0f48fb6eac62d24e04.css"
    integrity="sha256-bVheeNKMzhIA05&#43;xM8ku2D3wFzjach0PSPturGLSTgQ="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.6b00d96498d59caa0dbcf9b49d30d821915291f2ceb0e19248523c8607ff43fa.css"
    integrity="sha256-awDZZJjVnKoNvPm0nTDYIZFSkfLOsOGSSFI8hgf/Q/o="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.icofavicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/favicon.icoapple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon.icofavicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon.icofavicon-16x16.png" />

  <link rel="canonical" href="https://jamesmshihua.com/post/gd-linesearch/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js"
      integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8="
      crossorigin="anonymous"
    ></script>

  

  


  
  <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jamesmshihua.com/images/site-feature-image.png"/>

<meta name="twitter:title" content="Gradient Descent: Line Search"/>
<meta name="twitter:description" content="Gradient descent methods and their improvements."/>



  
  <meta property="og:title" content="Gradient Descent: Line Search" />
<meta property="og:description" content="Gradient descent methods and their improvements." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jamesmshihua.com/post/gd-linesearch/" /><meta property="og:image" content="https://jamesmshihua.com/images/site-feature-image.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-03-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-03-21T00:00:00+00:00" /><meta property="og:site_name" content="My blog" />
<meta property="og:see_also" content="https://jamesmshihua.com/post/sailboat-path-opt/" /><meta property="og:see_also" content="https://jamesmshihua.com/post/gd-quasi-newton/" />




  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Gradient Descent: Line Search",
        "headline": "Gradient Descent: Line Search",
        "alternativeHeadline": "",
        "description": "
      Gradient descent methods and their improvements.


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/jamesmshihua.com\/post\/gd-linesearch\/"
        },
        "author" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "creator" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "James Shihua"
        },
        "copyrightYear" : "2022",
        "dateCreated": "2022-03-21T00:00:00.00Z",
        "datePublished": "2022-03-21T00:00:00.00Z",
        "dateModified": "2022-03-21T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "James Shihua",
            "url": "https://jamesmshihua.com",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/jamesmshihua.com\/favicons\/favicon.icofavicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "https://jamesmshihua.com/images/site-feature-image.png"


      
      ]

    ,
        "url" : "https:\/\/jamesmshihua.com\/post\/gd-linesearch\/",
        "wordCount" : "1209",
        "genre" : [ ],
        "keywords" : [ 
      
      "optimisation"

    
      
        ,

      
      "mathematics"

    ]
    }
  </script>



</head>
<body
    
      class="body theme--light"

    
  >
    <div class="wrapper">
      <aside class="wrapper__sidebar"><div
  class="sidebar
    animated fadeInDown

  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      <h1 class="sidebar__introduction-title">
        <a href="/">My blog</a>
      </h1>
      <div class="sidebar__introduction-description">
        <p>James's imagination runs wild</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a href="https://www.linkedin.com/in/james-mingzhi-shihua-b4897a173/" rel="me" aria-label="Linkedin" title="Linkedin">
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="https://github.com/jamesmshihua/" rel="me" aria-label="GitHub" title="GitHub">
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="https://www.instagram.com/jameshmz/" rel="me" aria-label="instagram" title="instagram">
            <i class="fab fa-instagram fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li class="sidebar__list-item">
          <a href="mailto:ampule_dignity.05@icloud.com" rel="me" aria-label="e-mail" title="e-mail">
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        James Shihua
        2024


      
    </li>
    
      <li class="footer__item">
        <a
          class="link"
          href="/imprint/"
          
          title=""
        >
          imprint
        </a>
      </li>

    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.a3fd0638ada4e3cec287e49a604b4a828cfef33a669afe60b96575761fff5cf3.js"
    integrity="sha256-o/0GOK2k487Ch&#43;SaYEtKgoz&#43;8zpmmv5guWV1dh//XPM="
    crossorigin="anonymous"
  ></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] },
        TeX: { equationNumbers: { autoNumber: "all" } }
    });
    </script></div>
</aside>
      <main class="wrapper__main">
        <header class="header"><div
  class="
    animated fadeInDown

  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        <li class="nav__list-item">
          <a
            
            href="/"
            
            title=""
            >Home</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/post/"
            
            title=""
            >Posts</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/portfolio/"
            
            title=""
            >Portfolio</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/about/"
            
            title=""
            >About</a
          >
        </li>

      
        
        <li class="nav__list-item">
          <a
            
            href="/contact/"
            
            title=""
            >Contact</a
          >
        </li>

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>

      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown

    "
  >
    
    <div class="post__content">
      <h1>Gradient Descent: Line Search</h1>
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                21/3/2022


              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">6-minute read</span>
          </li>
        </ul>

      <h2 id="introduction-of-gradient-descent">Introduction of Gradient Descent</h2>
<p>Gradient Descent (GD) is an optimisation algorithm that does not need my introduction. For the sake of notation, the update formula of GD is</p>
<p>$$
\mathbf{x} _{k+1}=\mathbf{x} _{k}-\gamma _{k} \nabla F\left(\mathbf{x} _{k}\right),\quad n \geq 0,
\label{eqn:vanilla-gd}
$$</p>
<p>where $\mathbf{x}$ is the vector-valued <strong>design variable</strong> and $F\left(\mathbf{x}\right)$ is a multi-variable function representing your <strong>optimisation goal</strong>.</p>
<p>Despite its simplicity, this algorithm works well for many well-behaved primitive functions: gradient is neither too large nor too small, $F$ convex and $\nabla F$ Lipschitz, etc.. However, for complex engineering applications, where $\mathbf{x}$ can have hundreds of variables and $F$ may have many saddle points and drastically changing gradient, our naive assumptions may not hold true. We need to find some more efficient methods to converge to the optimal design point.</p>
<h2 id="line-search">Line Search</h2>
<h3 id="exact-line-search">Exact Line Search</h3>
<p>At a first glance, there is only one parameter you can tune in \eqref{eqn:vanilla-gd}, that is $\gamma$. By selecting $\gamma$, the magnitude of the gradient does not matter any more. An <strong>exact line search</strong> tries to minimise</p>
<p>$$
h(\alpha)=f\left(\mathbf{x}_ {k}+\alpha \mathbf{p}_ {k}\right),\quad\alpha \in \mathbb{R}_ {+},
$$</p>
<p>where $\mathbf{p}$ is the descent (search) direction and $\alpha$ is the step size we are trying to find. Then, update formula becomes</p>
<p>$$
\mathbf{x} _{k+1}=\mathbf{x} _{k}+\alpha\mathbf{p} _{k}.
$$</p>
<p>However, it is often that the search direction is calculated from the gradient, so</p>
<p>$$
\mathbf{p}=k\nabla F(\mathbf{x}).
$$</p>
<p>For example, we want to minimise the function of</p>
<p>$$
f(\mathbf{x})=\frac{1}{2} x_{1}^{2}+\frac{9}{2} x_{2}^{2}
\label{eqn:eg1}
$$</p>
<p>by gradient descent with line search. The direction of steepest descent is calculated by the gradient</p>
<p>$$
\mathbf{d} = -\nabla F(\mathbf{x}) = \begin{pmatrix}
-x_{1} \\
-9x_{2}
\end{pmatrix}.
$$</p>
<p>We also want to calculate $\alpha$ that gives the largest reduction of the function value</p>
<p>$$
\min_ {\alpha} f\left(\mathbf{x}-\alpha \nabla f\mathbf{x}\right)=\min_ {\alpha} \frac{1}{2}\left(x_ {1}-\alpha x_ {1}\right)^{2}+\frac{9}{2}\left(x_ {2}-9 \alpha x_ {2}\right)^{2}
$$</p>
<p>of which the optimal $\alpha$ is</p>
<p>$$
\alpha=\frac{x _{1}^{2}+81x _{2}^{2}}{x _{1}^{2}+729x _{2}^{2}}.
$$</p>
<p>This &ldquo;lowest&rdquo; point along a search direction is called the <strong>Cauchy point</strong>. The update formula should be</p>
<p>$$
\mathbf{x}_ {k+1}=
\mathbf{x}_ {k}+\frac{\left(x_ {1}\right)_ {k}^{2}+81\left(x_ {2}\right)_ {k}^{2}}{\left(x_ {1}\right)_ {k}^{2}+729\left(x_ {2}\right)_ {k}^{2}}
\begin{pmatrix}
-\left(x_ {1}\right)_ {k} \\
-9\left(x_ {2}\right)_ {k}
\end{pmatrix}.
$$</p>
<p>Our initial guess is at $x_{0}=\begin{pmatrix}9 &amp; 1\end{pmatrix}^{\mathrm{T}}$.</p>
<p>The plot and table show the convergence rate using this update formula for 55 iterations. Every orange circle is the Cauchy point along that search path.</p>
<figure class="small"><img src="/images/gdm1/gdm1.png"
         alt="image"/>
</figure>

<figure class="small"><img src="/images/gdm1/table1.png"
         alt="image"/>
</figure>

<p>This is very inefficient as we used 56 iterations to optimise this simple parabolic function and only achieved a tolerance of $\epsilon\sim 10^{-5}$. The &ldquo;zig-zag&rdquo; phenomenon is very common in gradient descent algorithms due to the anisotropic gradient in each directions. With that said, optimising</p>
<p>$$
f(\mathbf{x})=x_ 1^2+x_ 2^2
\label{eqn:eg2}
$$</p>
<p>will not have this problem.</p>
<h3 id="preconditioning">Preconditioning</h3>
<p>Now the natural question to ask is, can we turn \eqref{eqn:eg1} into \eqref{eqn:eg2}? We can do so by change of variables</p>
<p>$$
\begin{aligned}
&amp;x_{1}^{\prime}=x_{1} \\
&amp;x_{2}^{\prime}=3 x_{2}.
\end{aligned}
$$</p>
<p>and \eqref{eqn:eg1} can be re-written as</p>
<p>$$
\tilde{f}\left(x^{\prime}\right)=\frac{1}{2} x_ {1}^{\prime 2}+\frac{9}{2}\left(\frac{1}{3} x_ {2}^{\prime}\right)^{2}=\frac{1}{2} x_ {1}^{\prime 2}+\frac{1}{2} x_ {2}^{\prime 2}
$$</p>
<p>It is without doubt that in a single iteration, the line search method can find the global minimum, which is $(0,0)$.</p>
<figure class="small"><img src="/images/gdm1/gdm2.png"
         alt="image"/>
</figure>

<p>In summary, preconditioning the function makes the original elliptical (anisotropic) contour lines circular (isotropic).</p>
<p>Formally, the preconditioned steepest descent is</p>
<p>$$
\mathbf{x}_ {k+1}=\mathbf{x}_ {k}+\mathbf{H}_ {k}^{-1}\alpha_{k} \mathbf{d}_ {k}
$$</p>
<p>where $\mathbf{H}_{k}^{-1}$ is called the <strong>preconditioner</strong>, which must be <strong>symmetric positive definite</strong>. We will mention later that, in Newton&rsquo;s Method, $\mathbf{H}$ is the Hessian of $F$.</p>
<h3 id="inexact-line-search">Inexact Line Search</h3>
<p>Previously, we have performed an exact line search with preconditioning, with analytic gradient and preconditioner. However, these two pieces of information are not always available. In addition, exact line search induces another optimisation over the search path, and may be too computationally expensive to use in practical cases. Often, we use <strong>inexact line search</strong> instead. Inexact line search tries to &ldquo;loosely&rdquo; minimise the function value. In other words, as long as $f(x_{k+1})&lt;f(x_n)$, such step is a &ldquo;good choice&rdquo;.</p>
<p>It seems to be a good idea, however, does not necessarily converge. Suppose we want to optimise $y=x^2$ which is simple enough. Let&rsquo;s try two step sizes which <strong>strictly</strong> minimise the function value at the next iteration.</p>
<ul>
<li>The first choice is $\alpha_k=2+3(2^{-k-1})$. It can be analytically proven that $x_ {k}=(-1)^{k}\left(1+2^{-n}\right)$ and $f(x_ {k+1})&lt;f(x_ {k})$.</li>
<li>The second choice is $\alpha_k=2^{-k-1}$. It can also be analytically proven that $x_ {k}=1+2^{-n}$ and $f(x_ {k+1})&lt;f(x_ {k})$.</li>
</ul>
<figure class="small"><img src="/images/gdm1/gdm3.png"
         alt="image"/>
</figure>

<p>It is clear that for the first case, the function value is monotonically decreasing, however, for the first case, $\lim_{n\to\infty}x_n\nexists$ (does not converge), and for the second case, $\lim_{n\to\infty}x_n=1\neq0$ (converges to a wrong value).</p>
<h2 id="wolfe-conditions">Wolfe Conditions</h2>
<p>We want to systematically determine a proper step size instead of randomly guessing.</p>
<h3 id="first-wolfe-condition">First Wolfe Condition</h3>
<p>We have two ideas of choosing a step size as large as possible.</p>
<ul>
<li>If we use a large step, we want a large reduction in function value. \eqref{eqn:wolfe1-1} says, the reduction in function value should be at least proportional to the step size, by the proportionality constant $\gamma$.</li>
</ul>
<p>$$
f\left(\mathbf{x}_ {k}+\alpha_ {k} d_ {k}\right) \leq f\left(\mathbf{x}_ {k}\right)-\alpha_ {k} \gamma
\label{eqn:wolfe1-1}
$$</p>
<ul>
<li>If we have a large gradient, we want a large reduction in function value. \eqref{eqn:wolfe1-2} says, the proportionality constant $\gamma$ should be proportional to the gradient.</li>
</ul>
<p>$$
\gamma=-\beta_{1} \nabla f\left(\mathbf{x}_ {k}\right)^{T} \mathbf{d}_ {k},\quad 0&lt;\beta_1&lt;1
\label{eqn:wolfe1-2}
$$</p>
<p>Combine \eqref{eqn:wolfe1-1} and \eqref{eqn:wolfe1-2}, we have</p>
<p>$$
f\left(\mathbf{x}_ {k}+\alpha_ {k} \mathbf{d}_ {k}\right) \leq f\left(\mathbf{x}_ {k}\right)+\alpha_ {k} \beta_{1} \nabla f\left(\mathbf{x}_ {k}\right)^{T} \mathbf{d}_ {k},\quad 0&lt;\beta_1&lt;1
$$</p>
<p>It is important to note that both ends of the interval are <strong>excluded</strong>. Let&rsquo;s take a look the extreme cases.</p>
<ul>
<li>When $\beta_1=0$, we have $f\left(\mathbf{x}_ {k}+\alpha_ {k} \mathbf{d}_ {k}\right) \leq f\left(\mathbf{x}_ {k}\right)$. This is nothing different from the cases we studied previously, and this condition is not sufficient.</li>
<li>When $\beta_1=1$, we have $f\left(\mathbf{x}_ {k}+\alpha_ {k} \mathbf{d}_ {k}\right) \leq f\left(\mathbf{x}_ {k}\right)+\alpha_ {k} \nabla f\left(\mathbf{x}_ {k}\right)^{T} \mathbf{d}_ {k}$. Note that the RHS is now the tangent line of the function. If the function is convex, it is impossible to find an $\alpha_k$ that satisfies this condition.</li>
<li>The smaller the $\beta_1$ (less strict), the larger the step size is allowed.</li>
</ul>
<figure class="small"><img src="/images/gdm1/gdm4.png"
         alt="image"/><figcaption>
            <p>Illustration of first and second Wolfe conditions</p>
        </figcaption>
</figure>

<p>This figure shows that, if the starting point is $x=-1.5$, when $\beta_1=0$, $\alpha\le3$, but when $\beta_1=1$, $\alpha\nexists$!</p>
<h3 id="second-wolfe-condition">Second Wolfe Condition</h3>
<p>The first Wolfe condition avoids a too large step, while the second Wolfe condition avoids a too small step. We define the ratio of the change in directional derivative and the directional derivative itself.</p>
<p>$$
\beta_2=\frac{\nabla f(\mathbf{x} _k+\alpha \mathbf{d} _k)\mathbf{d} _k}{\nabla f(\mathbf{x} _k)\mathbf{d} _k}.
$$</p>
<p>When $\alpha=0$, $\beta_2=1$, because there is no movement. When $\alpha=\alpha^*$ so that it reaches the local minimum along the search direction, ${\nabla f(\mathbf{x} _k)\mathbf{d} _k}=0$ because the gradient is perpendicular to the search direction. Consequently, $\beta_2=0$</p>
<p>It is important to note that both ends of the interval are <strong>excluded</strong>. Let&rsquo;s take a look the extreme cases.</p>
<ul>
<li>When $\beta_2=0$, all step sizes that reduce the function value are accepted.</li>
<li>When $\beta_2=1$, only the step that reaches the local minimum along the search direction is selected.</li>
</ul>
<p>It is obvious that, for a sensible selection of $\alpha$, we need</p>
<p>$$
0&lt;\beta_1&lt;\beta_2&lt;1,
$$</p>
<p>called the validity of Wolfe conditions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This blog briefly introduces the concept of line search and Wolfe conditions. In another blog of this series, we will explore other gradient descent methods, such as Adams, etc..</p>



<h3>Posts in this Series</h3>
<ul>
  
    <li><a href="/post/sailboat-path-opt/">How Fast Can a Sailboat Go: A Constrained Optimization Approach</a></li>

  
    <li><a href="/post/gd-quasi-newton/">Gradient Descent: Newton and Quasi-Newton Method</a></li>

  
    <li><a href="/post/gd-linesearch/">Gradient Descent: Line Search</a></li>

  
</ul>
</div>
    <div class="post__footer">
      

      
        <span><a class="tag" href="/tags/optimisation/">optimisation</a><a class="tag" href="/tags/mathematics/">mathematics</a></span>




      
    </div>

    
  </div>


      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        James Shihua
        2024


      
    </li>
    
      <li class="footer__item">
        <a
          class="link"
          href="/imprint/"
          
          title=""
        >
          imprint
        </a>
      </li>

    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.a3fd0638ada4e3cec287e49a604b4a828cfef33a669afe60b96575761fff5cf3.js"
    integrity="sha256-o/0GOK2k487Ch&#43;SaYEtKgoz&#43;8zpmmv5guWV1dh//XPM="
    crossorigin="anonymous"
  ></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] },
        TeX: { equationNumbers: { autoNumber: "all" } }
    });
    </script></body>
</html>
